# Blog post 1 \(26-10-16\)

## First meeting with supervisor
I had my first meeting with my supervisor today - 26-10-16
We discussed:

1. The project proposal

2. What I have done so far?

3. What I plan to do next?


### 1. Project proposal

I had my project proposal early last week. The lectures who were listening were Mark Humphrys and Renaat Verbruggen. They were both generally quite positive in relation to my project.
They had a few remarks:

* Should I look into R? 
    * I am familiar with R as I used it in second year
    * R was created with data analysis in mind and handles large datasets well.
    * It is also a very graph orientated language with ggplot and may be helpful in this regard.
    * I am considering this and may use R for the more data analysis heavy sections of my project.
    
* To think of what factors I will be building my model around \(Time, where it was posted\)

* How will I be presenting this project?
    * I plan on having a very visual project presentation
    * Have lots of graphs showing what I found and to be able to show that I thought X going in and found Y
    
* Will I be ok with the underlying statistics of the project?
    * I am fairly sure that I should be ok with an stats I run accross and will be able to figure it out.
    * It is still a worry as maths is not my strongest quality, I will need to be wary of this.

* What purpose does this project have/Do advertisers use reddit?
    * I feel that this project will prove useful to advertisers hoping to use reddit to promote their brand
    * Even if this project proves to be of no use to adverisers I still feel confident in the methods which I will use \(Regression, log regression, SVM\) and the data analytical 
      side that it will be able to stand on its own if this proves an issue
     
### 2. What have I done so far?

* I have begun reading the Praw docs to find the most efficient way to pull data from reddit
    * The good:
        * Praw provides a method which pulls all posts between two time stamps
        * This method can get posts from specific subreddits or site wide
        * Posts are returned in chronological order
        * This is a batch call to the api and as such is fast \(60000 in an hour\), so I won't have to wait long to get up and running
    
    * The bad:
        * Due to api limitations the variable upvote_ratio and a posts comments are not returned using the above method. 
          Reading through the docs I beleive this is to save on bandwidth or something similar, I do however get the comments unique ID
        * I think that this is a very important variable as it will allow me to roughly figure out how many votes on a post were positive or negative
        * A call to get a single post returns the upovote\_ratio, so to get around this limitation I plan to take all comment\_ids and call for all comments individually.
        * Discussing this with my supervisor she suggested looking into threading or multi-core processing and I feel that 
          this is an excellent suggestion and plan to implement this work around using threading

* Began looking at research papers
    * I have collected several review papers which deal with the topic of Reddit popularity
    * I will likely take some of the methods used on board for use in my own project
    
### 3. What do I plan to do?
* I would like to look into threading to get around the reddit's api limitation using threading and make some sort of pipeline that 
    * Grab all info from posts and pass the ids off to to a seperate thread
    * The info will then get pre-processed and passed into off to another thread/process
    * In this thread the post will then get put into a database
    * While the above is happening the ids will have been passed into this thread where they will be stored in an array. I think option 1 suits PRAW3 better as PRAW4 requires OAuth tokens. But the second scenario suits 4 better as it only has a 1 second limitation as opposed to 3's 2 second limit This may work in one of two ways:
        1. If reddit's api allows threading then I can spilt this up into multiple threads with a __shared id array__. The threads will pull the post and then pass it to another thread which will find where the comment is stored in the database and slot in the ratio.
        2. If reddit's api does not support concurrent calls then this will mean that I will be able to use multiple machines which have different ip addresses but access to a shared array containing all submission ids. I may be fit to do this using different virtual machines as well.
    *__Make some sort of pipeline diagram in UML to describe this__
    This threading idea really requires more time to sink into my head so these are just fairly shallow thoughts on the issue at the minute, but I like the idea a lot. 
    However Marija warned against focusing on it too early as I would be better served figuring out the 1st model before jumping into this idea. So I need to put this idea on the backburner and not focus on it, but keep it in mind 

* I need to find more papers and read through them and write up some notes. Marija asked me to write a research plan for our next meeting, so that means I should look into how to properly structure that and what will be required for this.
* I also brought up the possiblity of using a neural network for some of the analysis and Marija warned against this, to focus on one method of machine learning. However if the project didn't have enough meat on the bones then I could pivot my project into a compare and contrast of different machine learning techniques \(Regression, neural networks, deep learing\). While I hope that it will be enough as stands I think have a saftey net is reassuring.
* I need to start getting data, but first I need to get a database to store it.
* I need to look into the best way to store the data before I can take any data
    * Traditional SQL relational database. I am already familiar with this form of SQL so I may use this
    * NOQL I am not familiar with this type of SQL but I may use this instead as the reddit ifo lends itself to a json like format
    * Either way I will probably be using the python wrapper SQLITE3 or PyMongo

### 4. Final Thoughts
* A few possible avenues of thought are:
    * Quantity V Quality poster, a who posts alot will eventually get a high scoring post. People who post frequently may also benefit from a recognition effect. Does this follow a poisson distribution?
    * Comment to vote ratio, easily digestible content will likely have a higher ratio of votes to comments much faster, less thought provoking, __more viral__, what easily digestable pictures break this mold? blue/black or white/gold dress?
    * Keep threading and parallelism in mind!
    * Look into using R for some data analytics sections